{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+VsOdhPYe0KnN6+vJVkPX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PauloCesarRSilva/using-azure-resources-to-translate/blob/main/Translate_Using_Azure_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzsC0beXXf5",
        "outputId": "01f173ff-be9e-4c34-e2d8-40e830774487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.19)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.1.143)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 openai langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "Gv0TDYLsZgfH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_url(url:str):\n",
        "  \"\"\"\n",
        "  Extract text data from url page\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  url:str\n",
        "    The URL page\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  text_clean:str\n",
        "    The text of the URL\n",
        "\n",
        "  \"\"\"\n",
        "  res = requests.get(url)\n",
        "\n",
        "  if res.status_code == 200:\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    for script_or_style in soup(['script', 'style']):\n",
        "      script_or_style.decompose()\n",
        "\n",
        "    text = soup.get_text(separator= ' ')\n",
        "\n",
        "    return text\n",
        "  else:\n",
        "    raise Exception (\n",
        "        f\"Failed to fetch URL. {res.status_code} reason: {res.reason}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "NmeP4YvFZ9FN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_text_from_url('https://medium.com/@paulocesar.r.silva/say-goodbye-to-slow-api-data-retrieval-with-for-loops-87a80fd1f7ae')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "W78ebUWeeb6r",
        "outputId": "5445f8fc-e7a2-485d-828a-14f35b0ec6c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Say Goodbye to Slow API Data Retrieval with for loops! Unlock Efficiency with Spark RDD Distributed Computing! | by Paulo Cesar Rodrigues da Silva | Medium Open in app Sign up Sign in Write Sign up Sign in Say Goodbye to Slow API Data Retrieval with for loops! Unlock Efficiency with Spark RDD Distributed Computing! A Journey from Slow Loops to Spark RDD Efficiency Paulo Cesar Rodrigues da Silva · Follow 6 min read · Mar 7, 2024 -- Listen Share Introduction For loops are cornerstone of programming, allowing us to perform many tasks efficiently. As technology has advanced, for loops have become more versatile and faster. However, when dealing with data and with the increase in data volume, we’ve started to notice their limitations, especially when dealing with large amounts of data. Imagine you have a task that involves processing huge amounts of data. This is where we encounter a problem. While for loops work fine for small tasks, they become slow and inefficient when dealing with massive datasets. As businesses aim to save time and money, and finding better ways to handle large amounts of data is crucial. In this article, we’ll explore why for loops struggle with big data and introduce a solution:  The use of Spark RDD . This technology promises faster processing time and better performance, helping businesses make the most of their data. The Problem Unveiled Picture this: The company has a job that fetches and retrieves data from an API. The catch? There are around 5000 records to process, and the API takes 1 to 2 seconds to respond to each request. Let’s assume that each record takes exactly one second, a traditional for loop would still take a whopping 5000 seconds to process all the data. That’s equivalent to 83.33 minutes! Just imagine the frustration of waiting for a single loop to plod through 5000 records, not to mention the financial strain on the company, which must foot the bill for a job that consumes over an hour of processing time each day for a single job. The Search for a Solution The solution to our data retrieval was right under my nose all along, yet it took a moment of insight to realize its potential. With some experimentation and guidance from  ChatGPT , I embarked on a journey to parallelize the execution of API calls — A solution I hadn’t thought of before. By using  Spark RDD , I uncovered the transformative power of  Spark RDD  for fetching data from APIs. By  distributing  the workload  across multiple   processing units , I achieved a solution that was not only highly scalable but also inherently resilient. The results were amazing! Our job, which used to take hours, now finishes in a fraction of the time — 90% less, to be exact. This not only saves us time and money but also means we can handle more work in the future. Embracing Spark RDD : Spark RDD stands for  Resilient Distributed Dataset , a Spark object with a unique and singular difference compared to Python object: It is distributed. Through RDD, you can enjoy and make use of the best of what a distributed computing has to offer. For instance, look at the spark ecosystem: Picture of Spark Ecosystem If you run your Python code, it operates at the  driver level , representing just the surface of Spark’s capabilities. However, when working with Spark RDDs, you dive deeper into the  Spark Core , engaging with Spark objects at the  core level . Here, distributed computing and parallelism take precedence over handling Python objects. Picture of how spark work with parallel processing. Results and Benefits : Let’s assume that I must make API calls on about 5000 registers each. Running a python for loop in spark, this is what happens: As you can see, the code runs only at driver level. No parallelism or distribution happens here. Using Python for loop: · The code is executed sequentially in a single machine · Each iteration process one element at a time Considering that I process  each element at a time , and each element takes 1 second to process, having 5000 registers  will take 5000   seconds  to finish, that is  83.33 minutes ! This is far from efficient and consumes considerable time. However, what if I could parallelize the operation, enabling simultaneous API calls across multiple computers? When you create a Spark RDD from your data, the Spark: · Distributes the computation across multiple nodes in a cluster · Processes data in parallel, applying the transformation function to each element of the RDD partition independently. As a result, you’re not just performing one operation at a time, but processing  multiple items simultaneously , leading to significantly reduced execution time due to the utilization of multiple machines. With the ability to process  4 items simultaneously  within a single second, you  complete  the processing  in 1250 seconds , that is  20.83 minutes ! This represents a remarkable  reduction  of 75% in  execution time , assuming your cluster consists of 4 workers. The code If you have a code like this: list_data # the list of your data list_data_new = [] def get_api_call(item):   # operation to fetch data from api for item in list_data:   data = get_api_call(item)   list_data_new.append(data) Just replace your for loop with this: def get_api_call(item):   # operation to fetch data from api # turn the `list_data` python list into an RDD (Spark object)  # so we can make use of spark resources like parallelism list_data_rdd = sc.parallelize(list_data) # once we are now dealing with spark object, we can perform operations using  # parallelism. Here, spark is serializing python function  # `get_api_call` into bitecode and sending to worker nodes # So, the mapping is applied on each partition of the RDD on the worker nodes  # independently result_rdd = list_data_rdd.map(get_api_call) # finally, we do a `collect` to return the data back to the driver as  # python list list_data_new = result_rdd.collect() Caution : The use of  .collect() retrieves RDD data back to the driver, potentially causing performance issues, especially with large datasets. Alternatively, if you are not performing any complex operation on your data, consider converting the RDD into a DataFrame. But if my function has two or more parameters? The approach is quite simple: def get_api_call(item_1, item_2):   # operation to fetch data from api # considering that your source is a dataframe, create a list with a pair of  # tuples. For this you can create using list comprehension list_data_pairs = [(list_data_df[\"item_1\"][i], list_data_df[\"item_2\"][i]) for i in range(len(list_data_df))] list_data_rdd = sc.parallelize(list_data_pairs) # on rdd.map, just make a simple change so you can map your parameters # accordingly results_rdd = list_data_rdd.map(lambda pair: get_api_call(*pair)) list_data_new = result_rdd.collect() Conclusion : In conclusion, leveraging Spark RDD for distributed computing can significantly enhance the efficiency of data processing tasks, particularly when dealing with large datasets. By parallelizing operations across multiple nodes in a cluster, Spark RDD enables faster execution times and better utilization of resources. References Spark Under The Hood : Partition. Spark is a distributed computing engine… | by Thejas Babu | Medium PySpark: Everything you need to know! | by Sharan Harsoor | AI Mind Spark Python Databricks Distributed Systems Parallel Computing -- -- Follow Written by  Paulo Cesar Rodrigues da Silva 6 Followers · 3 Following Passionate about data and technology. Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
        "\n",
        "client = AzureChatOpenAI(\n",
        "    azure_endpoint='your-endpoint.openai.azure.com/',\n",
        "    api_key='your-key',\n",
        "    api_version='2024-02-15-preview',\n",
        "    deployment_name='gpt-4o-mini',\n",
        "    max_retries=0\n",
        ")\n",
        "\n",
        "def translate_article(text:str, lang:str):\n",
        "  \"\"\"\n",
        "  Translate article format text\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text:str\n",
        "    The text of the article\n",
        "  lang:str\n",
        "    The language the text will be translated\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "      (\"system\", \"Você atua como tradutor de textos\"),\n",
        "      (\"user\", f\"Traduza o {text} para o idioma {lang}\")\n",
        "  ]\n",
        "\n",
        "  response = client.invoke(messages)\n",
        "  #print(response.content)\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "3cn9fe33gm85"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_article('Hello World', 'Francês')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g7XpDp7IpFH3",
        "outputId": "da4839ce-a20d-4159-a713-448d3cfeaf4f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Hello World\" em francês é \"Bonjour le monde\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://medium.com/@paulocesar.r.silva/say-goodbye-to-slow-api-data-retrieval-with-for-loops-87a80fd1f7ae'\n",
        "text = extract_text_from_url(url)\n",
        "translate_article(text, 'português')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "TkR1mvuOqGoQ",
        "outputId": "c15e6911-1abb-4a21-82b1-6b7e15d56932"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Diga adeus à recuperação lenta de dados da API com loops for! Desbloqueie a eficiência com o Spark RDD de computação distribuída! | por Paulo Cesar Rodrigues da Silva | Medium\\n\\nDiga adeus à recuperação lenta de dados da API com loops for! Desbloqueie a eficiência com o Spark RDD de computação distribuída! Uma jornada dos loops lentos à eficiência do Spark RDD\\n\\nPaulo Cesar Rodrigues da Silva · Seguir 6 min de leitura · 7 de março de 2024\\n\\n-- Ouvir Compartilhar\\n\\nIntrodução\\n\\nOs loops for são a pedra angular da programação, permitindo que realizemos muitas tarefas de forma eficiente. À medida que a tecnologia avançou, os loops for se tornaram mais versáteis e rápidos. No entanto, ao lidar com dados e com o aumento do volume de dados, começamos a notar suas limitações, especialmente ao trabalhar com grandes quantidades de dados. Imagine que você tem uma tarefa que envolve processar enormes quantidades de dados. É aqui que encontramos um problema. Embora os loops for funcionem bem para pequenas tarefas, eles se tornam lentos e ineficientes ao lidar com conjuntos de dados massivos. À medida que as empresas buscam economizar tempo e dinheiro, encontrar maneiras melhores de lidar com grandes quantidades de dados é crucial. Neste artigo, exploraremos por que os loops for têm dificuldades com big data e apresentaremos uma solução: o uso do Spark RDD. Essa tecnologia promete um tempo de processamento mais rápido e melhor desempenho, ajudando as empresas a aproveitarem ao máximo seus dados.\\n\\nO problema revelado\\n\\nImagine isso: a empresa tem um trabalho que busca e recupera dados de uma API. O problema? Existem cerca de 5000 registros para processar, e a API leva de 1 a 2 segundos para responder a cada solicitação. Vamos supor que cada registro leve exatamente um segundo; um loop for tradicional ainda levaria impressionantes 5000 segundos para processar todos os dados. Isso equivale a 83,33 minutos! Apenas imagine a frustração de esperar que um único loop percorra 5000 registros, sem mencionar a pressão financeira sobre a empresa, que deve arcar com os custos de um trabalho que consome mais de uma hora de tempo de processamento por dia para uma única tarefa.\\n\\nA busca por uma solução\\n\\nA solução para nossa recuperação de dados estava bem diante dos meus olhos o tempo todo, mas foi necessário um momento de insight para perceber seu potencial. Com alguns experimentos e orientação do ChatGPT, embarquei em uma jornada para paralelizar a execução das chamadas da API — uma solução que eu nunca havia considerado antes. Ao usar o Spark RDD, descobri o poder transformador do Spark RDD para buscar dados de APIs. Ao distribuir a carga de trabalho entre várias unidades de processamento, consegui uma solução que não apenas era altamente escalável, mas também inerentemente resiliente. Os resultados foram surpreendentes! Nosso trabalho, que costumava levar horas, agora termina em uma fração do tempo — 90% menos, para ser exato. Isso não apenas nos economiza tempo e dinheiro, mas também significa que podemos lidar com mais trabalho no futuro.\\n\\nAdotando o Spark RDD:\\n\\nSpark RDD significa Resilient Distributed Dataset, um objeto Spark com uma diferença única e singular em comparação com um objeto Python: ele é distribuído. Através do RDD, você pode aproveitar e fazer uso do melhor que a computação distribuída tem a oferecer. Por exemplo, veja o ecossistema Spark:\\n\\n[Imagem do Ecossistema Spark]\\n\\nSe você executar seu código Python, ele opera no nível do driver, representando apenas a superfície das capacidades do Spark. No entanto, ao trabalhar com Spark RDDs, você mergulha mais fundo no Spark Core, interagindo com objetos Spark no nível central. Aqui, a computação distribuída e o paralelismo têm prioridade sobre o manuseio de objetos Python.\\n\\n[Imagem de como o Spark funciona com processamento paralelo.]\\n\\nResultados e Benefícios:\\n\\nVamos supor que eu precise fazer chamadas de API em cerca de 5000 registros cada. Executando um loop for em Python no Spark, isso é o que acontece: Como você pode ver, o código é executado apenas no nível do driver. Nenhum paralelismo ou distribuição ocorre aqui.\\n\\nUsando o loop for em Python:\\n- O código é executado sequencialmente em uma única máquina.\\n- Cada iteração processa um elemento de cada vez.\\n\\nConsiderando que eu processo cada elemento de cada vez, e cada elemento leva 1 segundo para processar, ter 5000 registros levará 5000 segundos para terminar, ou seja, 83,33 minutos! Isso está longe de ser eficiente e consome um tempo considerável. No entanto, e se eu pudesse paralelizar a operação, permitindo chamadas de API simultâneas em vários computadores? Quando você cria um Spark RDD a partir dos seus dados, o Spark:\\n- Distribui a computação entre vários nós em um cluster.\\n- Processa dados em paralelo, aplicando a função de transformação a cada elemento da partição RDD independentemente.\\n\\nComo resultado, você não está apenas realizando uma operação por vez, mas processando múltiplos itens simultaneamente, levando a um tempo de execução significativamente reduzido devido à utilização de várias máquinas. Com a capacidade de processar 4 itens simultaneamente em um único segundo, você completa o processamento em 1250 segundos, ou seja, 20,83 minutos! Isso representa uma redução notável de 75% no tempo de execução, assumindo que seu cluster consiste em 4 trabalhadores.\\n\\nO código\\n\\nSe você tiver um código assim:\\n\\n```python\\nlist_data  # a lista dos seus dados\\nlist_data_new = []\\n\\ndef get_api_call(item):\\n    # operação para buscar dados da API\\n    for item in list_data:\\n        data = get_api_call(item)\\n        list_data_new.append(data)\\n```\\n\\nBasta substituir seu loop for por isso:\\n\\n```python\\ndef get_api_call(item):\\n    # operação para buscar dados da API\\n    # transformar a lista `list_data` em um RDD (objeto Spark) \\n    # para que possamos fazer uso dos recursos do Spark, como paralelismo\\n    list_data_rdd = sc.parallelize(list_data)\\n    # agora que estamos lidando com um objeto Spark, podemos realizar operações usando \\n    # paralelismo. Aqui, o Spark está serializando a função Python \\n    # `get_api_call` em bytecode e enviando para os nós trabalhadores\\n    # Assim, o mapeamento é aplicado em cada partição do RDD nos nós trabalhadores\\n    # independentemente\\n    result_rdd = list_data_rdd.map(get_api_call)\\n    # finalmente, fazemos um `collect` para retornar os dados de volta ao driver como \\n    # lista Python\\n    list_data_new = result_rdd.collect()\\n```\\n\\nCuidado: O uso de `.collect()` recupera os dados do RDD de volta ao driver, podendo causar problemas de desempenho, especialmente com grandes conjuntos de dados. Alternativamente, se você não estiver realizando nenhuma operação complexa em seus dados, considere converter o RDD em um DataFrame.\\n\\nMas e se minha função tiver dois ou mais parâmetros?\\n\\nA abordagem é bastante simples:\\n\\n```python\\ndef get_api_call(item_1, item_2):\\n    # operação para buscar dados da API\\n    # considerando que sua fonte é um DataFrame, crie uma lista com um par de \\n    # tuplas. Para isso, você pode usar a compreensão de listas\\n    list_data_pairs = [(list_data_df[\"item_1\"][i], list_data_df[\"item_2\"][i]) for i in range(len(list_data_df))]\\n    list_data_rdd = sc.parallelize(list_data_pairs)\\n    # no rdd.map, basta fazer uma simples alteração para que você possa mapear seus parâmetros \\n    # de acordo\\n    results_rdd = list_data_rdd.map(lambda pair: get_api_call(*pair))\\n    list_data_new = results_rdd.collect()\\n```\\n\\nConclusão:\\n\\nEm conclusão, aproveitar o Spark RDD para computação distribuída pode melhorar significativamente a eficiência das tarefas de processamento de dados, particularmente ao lidar com grandes conjuntos de dados. Ao paralelizar operações entre vários nós em um cluster, o Spark RDD permite tempos de execução mais rápidos e melhor utilização de recursos.\\n\\nReferências:\\n- Spark Under The Hood: Partition. Spark é um motor de computação distribuída… | por Thejas Babu | Medium\\n- PySpark: Tudo que você precisa saber! | por Sharan Harsoor | AI Mind\\n- Spark Python Databricks Sistemas Distribuídos Computação Paralela\\n\\n--\\n\\nSeguir\\n\\nEscrito por Paulo Cesar Rodrigues da Silva\\n6 Seguidores · 3 Seguindo\\nApaixonado por dados e tecnologia.\\n\\nSeguir\\n\\nNenhuma resposta ainda\\n\\nAjuda\\nStatus\\nSobre\\nCarreiras\\nImprensa\\nBlog\\nPrivacidade\\nTermos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diga adeus à recuperação lenta de dados da API com loops for! Desbloqueie a eficiência com o Spark RDD de computação distribuída! | por Paulo Cesar Rodrigues da Silva | Medium\n",
        "\n",
        "Diga adeus à recuperação lenta de dados da API com loops for! Desbloqueie a eficiência com o Spark RDD de computação distribuída! Uma jornada dos loops lentos à eficiência do Spark RDD\n",
        "\n",
        "Paulo Cesar Rodrigues da Silva · Seguir 6 min de leitura · 7 de março de 2024\n",
        "\n",
        "-- Ouvir Compartilhar\n",
        "\n",
        "Introdução\n",
        "\n",
        "Os loops for são a pedra angular da programação, permitindo que realizemos muitas tarefas de forma eficiente. À medida que a tecnologia avançou, os loops for se tornaram mais versáteis e rápidos. No entanto, ao lidar com dados e com o aumento do volume de dados, começamos a notar suas limitações, especialmente ao trabalhar com grandes quantidades de dados. Imagine que você tem uma tarefa que envolve processar enormes quantidades de dados. É aqui que encontramos um problema. Embora os loops for funcionem bem para pequenas tarefas, eles se tornam lentos e ineficientes ao lidar com conjuntos de dados massivos. À medida que as empresas buscam economizar tempo e dinheiro, encontrar maneiras melhores de lidar com grandes quantidades de dados é crucial. Neste artigo, exploraremos por que os loops for têm dificuldades com big data e apresentaremos uma solução: o uso do Spark RDD. Essa tecnologia promete um tempo de processamento mais rápido e melhor desempenho, ajudando as empresas a aproveitarem ao máximo seus dados.\n",
        "\n",
        "O problema revelado\n",
        "\n",
        "Imagine isso: a empresa tem um trabalho que busca e recupera dados de uma API. O problema? Existem cerca de 5000 registros para processar, e a API leva de 1 a 2 segundos para responder a cada solicitação. Vamos supor que cada registro leve exatamente um segundo; um loop for tradicional ainda levaria impressionantes 5000 segundos para processar todos os dados. Isso equivale a 83,33 minutos! Apenas imagine a frustração de esperar que um único loop percorra 5000 registros, sem mencionar a pressão financeira sobre a empresa, que deve arcar com os custos de um trabalho que consome mais de uma hora de tempo de processamento por dia para uma única tarefa.\n",
        "\n",
        "A busca por uma solução\n",
        "\n",
        "A solução para nossa recuperação de dados estava bem diante dos meus olhos o tempo todo, mas foi necessário um momento de insight para perceber seu potencial. Com alguns experimentos e orientação do ChatGPT, embarquei em uma jornada para paralelizar a execução das chamadas da API — uma solução que eu nunca havia considerado antes. Ao usar o Spark RDD, descobri o poder transformador do Spark RDD para buscar dados de APIs. Ao distribuir a carga de trabalho entre várias unidades de processamento, consegui uma solução que não apenas era altamente escalável, mas também inerentemente resiliente. Os resultados foram surpreendentes! Nosso trabalho, que costumava levar horas, agora termina em uma fração do tempo — 90% menos, para ser exato. Isso não apenas nos economiza tempo e dinheiro, mas também significa que podemos lidar com mais trabalho no futuro.\n",
        "\n",
        "Adotando o Spark RDD:\n",
        "\n",
        "Spark RDD significa Resilient Distributed Dataset, um objeto Spark com uma diferença única e singular em comparação com um objeto Python: ele é distribuído. Através do RDD, você pode aproveitar e fazer uso do melhor que a computação distribuída tem a oferecer. Por exemplo, veja o ecossistema Spark:\n",
        "\n",
        "[Imagem do Ecossistema Spark]\n",
        "\n",
        "Se você executar seu código Python, ele opera no nível do driver, representando apenas a superfície das capacidades do Spark. No entanto, ao trabalhar com Spark RDDs, você mergulha mais fundo no Spark Core, interagindo com objetos Spark no nível central. Aqui, a computação distribuída e o paralelismo têm prioridade sobre o manuseio de objetos Python.\n",
        "\n",
        "[Imagem de como o Spark funciona com processamento paralelo.]\n",
        "\n",
        "Resultados e Benefícios:\n",
        "\n",
        "Vamos supor que eu precise fazer chamadas de API em cerca de 5000 registros cada. Executando um loop for em Python no Spark, isso é o que acontece: Como você pode ver, o código é executado apenas no nível do driver. Nenhum paralelismo ou distribuição ocorre aqui.\n",
        "\n",
        "Usando o loop for em Python:\n",
        "- O código é executado sequencialmente em uma única máquina.\n",
        "- Cada iteração processa um elemento de cada vez.\n",
        "\n",
        "Considerando que eu processo cada elemento de cada vez, e cada elemento leva 1 segundo para processar, ter 5000 registros levará 5000 segundos para terminar, ou seja, 83,33 minutos! Isso está longe de ser eficiente e consome um tempo considerável. No entanto, e se eu pudesse paralelizar a operação, permitindo chamadas de API simultâneas em vários computadores? Quando você cria um Spark RDD a partir dos seus dados, o Spark:\n",
        "- Distribui a computação entre vários nós em um cluster.\n",
        "- Processa dados em paralelo, aplicando a função de transformação a cada elemento da partição RDD independentemente.\n",
        "\n",
        "Como resultado, você não está apenas realizando uma operação por vez, mas processando múltiplos itens simultaneamente, levando a um tempo de execução significativamente reduzido devido à utilização de várias máquinas. Com a capacidade de processar 4 itens simultaneamente em um único segundo, você completa o processamento em 1250 segundos, ou seja, 20,83 minutos! Isso representa uma redução notável de 75% no tempo de execução, assumindo que seu cluster consiste em 4 trabalhadores.\n",
        "\n",
        "O código\n",
        "\n",
        "Se você tiver um código assim:\n",
        "\n",
        "```python\n",
        "list_data  # a lista dos seus dados\n",
        "list_data_new = []\n",
        "\n",
        "def get_api_call(item):\n",
        "    # operação para buscar dados da API\n",
        "    for item in list_data:\n",
        "        data = get_api_call(item)\n",
        "        list_data_new.append(data)\n",
        "```\n",
        "\n",
        "Basta substituir seu loop for por isso:\n",
        "\n",
        "```python\n",
        "def get_api_call(item):\n",
        "    # operação para buscar dados da API\n",
        "    # transformar a lista `list_data` em um RDD (objeto Spark)\n",
        "    # para que possamos fazer uso dos recursos do Spark, como paralelismo\n",
        "    list_data_rdd = sc.parallelize(list_data)\n",
        "    # agora que estamos lidando com um objeto Spark, podemos realizar operações usando\n",
        "    # paralelismo. Aqui, o Spark está serializando a função Python\n",
        "    # `get_api_call` em bytecode e enviando para os nós trabalhadores\n",
        "    # Assim, o mapeamento é aplicado em cada partição do RDD nos nós trabalhadores\n",
        "    # independentemente\n",
        "    result_rdd = list_data_rdd.map(get_api_call)\n",
        "    # finalmente, fazemos um `collect` para retornar os dados de volta ao driver como\n",
        "    # lista Python\n",
        "    list_data_new = result_rdd.collect()\n",
        "```\n",
        "\n",
        "Cuidado: O uso de `.collect()` recupera os dados do RDD de volta ao driver, podendo causar problemas de desempenho, especialmente com grandes conjuntos de dados. Alternativamente, se você não estiver realizando nenhuma operação complexa em seus dados, considere converter o RDD em um DataFrame.\n",
        "\n",
        "Mas e se minha função tiver dois ou mais parâmetros?\n",
        "\n",
        "A abordagem é bastante simples:\n",
        "\n",
        "```python\n",
        "def get_api_call(item_1, item_2):\n",
        "    # operação para buscar dados da API\n",
        "    # considerando que sua fonte é um DataFrame, crie uma lista com um par de\n",
        "    # tuplas. Para isso, você pode usar a compreensão de listas\n",
        "    list_data_pairs = [(list_data_df[\"item_1\"][i], list_data_df[\"item_2\"][i]) for i in range(len(list_data_df))]\n",
        "    list_data_rdd = sc.parallelize(list_data_pairs)\n",
        "    # no rdd.map, basta fazer uma simples alteração para que você possa mapear seus parâmetros\n",
        "    # de acordo\n",
        "    results_rdd = list_data_rdd.map(lambda pair: get_api_call(*pair))\n",
        "    list_data_new = results_rdd.collect()\n",
        "```\n",
        "\n",
        "Conclusão:\n",
        "\n",
        "Em conclusão, aproveitar o Spark RDD para computação distribuída pode melhorar significativamente a eficiência das tarefas de processamento de dados, particularmente ao lidar com grandes conjuntos de dados. Ao paralelizar operações entre vários nós em um cluster, o Spark RDD permite tempos de execução mais rápidos e melhor utilização de recursos.\n",
        "\n",
        "Referências:\n",
        "- Spark Under The Hood: Partition. Spark é um motor de computação distribuída… | por Thejas Babu | Medium\n",
        "- PySpark: Tudo que você precisa saber! | por Sharan Harsoor | AI Mind\n",
        "- Spark Python Databricks Sistemas Distribuídos Computação Paralela\n",
        "\n",
        "--\n",
        "\n",
        "Seguir\n",
        "\n",
        "Escrito por Paulo Cesar Rodrigues da Silva\n",
        "6 Seguidores · 3 Seguindo\n",
        "Apaixonado por dados e tecnologia.\n",
        "\n",
        "Seguir\n",
        "\n",
        "Nenhuma resposta ainda\n",
        "\n",
        "Ajuda\n",
        "Status\n",
        "Sobre\n",
        "Carreiras\n",
        "Imprensa\n",
        "Blog\n",
        "Privacidade\n",
        "Termos"
      ],
      "metadata": {
        "id": "k5a2q8qbtQzs"
      }
    }
  ]
}